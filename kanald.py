import requests
from bs4 import BeautifulSoup
import json
import os
import re
from tqdm import tqdm
import urllib3
import subprocess
from datetime import datetime

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- AYARLAR ---
BASE_URL = "https://www.kanald.com.tr"
ROOT_DIR = "kanald"
DIRS = {
    "series": os.path.join(ROOT_DIR, "dizi"),
    "programs": os.path.join(ROOT_DIR, "program")
}

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": BASE_URL
}

def run_command(command):
    try:
        subprocess.run(command, shell=True, check=False, stdout=subprocess.DEVNULL)
    except: pass

def get_stream_url(page_url):
    """Video sayfasÄ±ndan m3u8 linkini (Zorlayarak) Ã§eker"""
    try:
        r = requests.get(page_url, headers=HEADERS, verify=False, timeout=10)
        html = r.text
        
        # YÃ–NTEM 1: Standart "Path":"...m3u8" yapÄ±sÄ± (JSON iÃ§inde)
        # Regex: "Path":"(https:.*?\.m3u8.*?)"
        match = re.search(r'"Path":"(https:[^"]*?\.m3u8[^"]*?)"', html)
        if match: return match.group(1).replace("\\/", "/")

        # YÃ–NTEM 2: data-media-sources attribute'u
        match2 = re.search(r"data-media-sources='(.*?)'", html)
        if match2:
            try:
                data = json.loads(match2.group(1))
                if "Hls" in data and "Path" in data["Hls"]:
                    return data["Hls"]["Path"]
            except: pass
            
        # YÃ–NTEM 3: Secure HLS Path
        match3 = re.search(r'"SecurePath":"(https:[^"]*?\.m3u8[^"]*?)"', html)
        if match3: return match3.group(1).replace("\\/", "/")

        # YÃ–NTEM 4: Basit dÃ¼z metin arama (Son Ã§are)
        match4 = re.search(r'(https:\/\/kanald[^"\']*?\.m3u8[^"\']*?)', html)
        if match4: return match4.group(1).replace("\\/", "/")

    except: pass
    return None

def get_episodes(show_url, show_name):
    episodes = []
    page = 1
    bolumler_url = show_url + "/bolumler"
    print(f"   ðŸ”Ž {show_name} bÃ¶lÃ¼mleri taranÄ±yor...")
    
    # Max 50 sayfa (Sonsuz dÃ¶ngÃ¼yÃ¼ Ã¶nlemek iÃ§in)
    while page < 50:
        try:
            target_url = f"{bolumler_url}?page={page}"
            r = requests.get(target_url, headers=HEADERS, verify=False, timeout=10)
            soup = BeautifulSoup(r.content, "html.parser")
            
            cards = soup.select(".listing-holder .item")
            if not cards: break
            
            # Bu sayfada yeni bÃ¶lÃ¼m bulduk mu?
            found_in_page = 0
            
            for card in cards:
                try:
                    a_tag = card.find("a")
                    if not a_tag: continue
                    link = BASE_URL + a_tag.get("href")
                    
                    title_tag = card.find("h3") or card.find("img")
                    title = title_tag.get_text(strip=True) if title_tag else "Bolum"
                    if not title and title_tag.name == "img": title = title_tag.get("alt")
                    
                    img_tag = card.find("img")
                    img = img_tag.get("data-src") or img_tag.get("src") if img_tag else ""
                    
                    # Video Linkini Ã‡ek
                    stream = get_stream_url(link)
                    
                    if stream:
                        episodes.append({"name": title, "img": img, "stream_url": stream})
                        found_in_page += 1
                except: continue
            
            if found_in_page == 0: 
                # EÄŸer sayfa 1 boÅŸsa, belki direkt ana sayfada videolar vardÄ±r
                if page == 1: break 
                else: break
            
            page += 1
            
        except: break

    print(f"   âœ… Toplam {len(episodes)} oynatÄ±labilir bÃ¶lÃ¼m bulundu.")
    return episodes

def collect_shows(category_url):
    print(f"ðŸŒ Kategori TaranÄ±yor: {category_url}")
    shows = []
    try:
        r = requests.get(category_url, headers=HEADERS, verify=False, timeout=15)
        soup = BeautifulSoup(r.content, "html.parser")
        cards = soup.select(".listing-holder .item, .program-list .item")
        
        for card in cards:
            a = card.find("a")
            if not a: continue
            url = BASE_URL + a.get("href")
            
            t_tag = card.find("h3") or card.find("img")
            name = t_tag.get_text(strip=True) if t_tag else "Bilinmeyen"
            if not name and t_tag.name == "img": name = t_tag.get("alt")
            
            img_tag = card.find("img")
            img = img_tag.get("data-src") or img_tag.get("src") if img_tag else ""
            
            shows.append({"name": name, "url": url, "img": img})
    except: pass
    print(f"âœ… {len(shows)} dizi/program bulundu.")
    return shows

def main():
    for d in DIRS.values(): os.makedirs(d, exist_ok=True)
    
    targets = [
        {"url": f"{BASE_URL}/diziler", "type": "series"},
        {"url": f"{BASE_URL}/programlar", "type": "programs"}
    ]
    
    all_data = {"series": [], "programs": []}
    
    for t in targets:
        shows = collect_shows(t["url"])
        
        for show in tqdm(shows, desc=f"{t['type']} Ä°ÅŸleniyor"):
            episodes = get_episodes(show["url"], show["name"])
            if episodes:
                show_data = {"name": show["name"], "img": show["img"], "episodes": episodes}
                slug = re.sub(r'[^a-z0-9-]', '', show['name'].lower().replace(" ", "-").replace("Ã§","c").replace("ÄŸ","g").replace("Ä±","i").replace("Ã¶","o").replace("ÅŸ","s").replace("Ã¼","u"))
                
                target_dir = DIRS[t["type"]]
                # JSON
                with open(os.path.join(target_dir, f"{slug}.json"), "w", encoding="utf-8") as f:
                    json.dump(show_data, f, ensure_ascii=False, indent=4)
                # M3U
                with open(os.path.join(target_dir, f"{slug}.m3u"), "w", encoding="utf-8") as f:
                    f.write("#EXTM3U\n")
                    for ep in episodes:
                        f.write(f'#EXTINF:-1 tvg-logo="{ep["img"]}",{ep["name"]}\n{ep["stream_url"]}\n')
                
                all_data[t["type"]].append(show_data)

    # Toplu Dosyalar
    print("\nðŸ“¦ Toplu listeler oluÅŸturuluyor...")
    for key, data in all_data.items():
        if not data: continue
        with open(os.path.join(ROOT_DIR, f"kanald-{key}.json"), "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        with open(os.path.join(ROOT_DIR, f"kanald-{key}.m3u"), "w", encoding="utf-8") as f:
            f.write("#EXTM3U\n")
            for show in data:
                for ep in show["episodes"]:
                    f.write(f'#EXTINF:-1 tvg-logo="{ep["img"]}" group-title="{show["name"]}",{ep["name"]}\n{ep["stream_url"]}\n')

    # GITHUB
    print("\nðŸš€ GitHub'a YÃ¼kleniyor...")
    run_command("git add --all")
    run_command("git add kanald/*")
    run_command(f'git commit -m "KanalD Guncelleme {datetime.now().strftime("%d-%m")}"')
    run_command("git push")
    print("âœ… Ä°ÅŸlem TamamlandÄ±.")

if __name__ == "__main__":
    main()

